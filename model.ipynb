{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = 'shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(sorted(set(data)))}\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_encoded = np.array([stoi[ch] for ch in train_data], dtype=np.uint16)\n",
    "val_data_encoded = np.array([stoi[ch] for ch in val_data], dtype=np.uint16)\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "n_embd = 32\n",
    "batch_size = 8\n",
    "block_size = 8\n",
    "steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data_encoded\n",
    "    elif split == 'val':\n",
    "        data = val_data_encoded\n",
    "    else:\n",
    "        raise ValueError('split must be either train or val')\n",
    "    start_idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idx])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idx])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[58,  6,  1, 39, 52, 42,  1, 57],\n",
       "         [27, 52, 43,  1, 46, 43, 39, 60],\n",
       "         [ 1, 43, 52, 42, 59, 56, 43,  1],\n",
       "         [53,  1, 61, 46, 47, 54,  1, 58],\n",
       "         [63, 53, 59, 56,  1, 41, 53, 59],\n",
       "         [ 1, 46, 53, 50, 63,  1, 20, 39],\n",
       "         [ 1, 24, 53, 56, 42,  1, 13, 52],\n",
       "         [ 1, 61, 53, 56, 58, 46, 47, 50]]),\n",
       " tensor([[ 6,  1, 39, 52, 42,  1, 57, 53],\n",
       "         [52, 43,  1, 46, 43, 39, 60, 43],\n",
       "         [43, 52, 42, 59, 56, 43,  1, 58],\n",
       "         [ 1, 61, 46, 47, 54,  1, 58, 46],\n",
       "         [53, 59, 56,  1, 41, 53, 59, 52],\n",
       "         [46, 53, 50, 63,  1, 20, 39, 56],\n",
       "         [24, 53, 56, 42,  1, 13, 52, 45],\n",
       "         [61, 53, 56, 58, 46, 47, 50, 63]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = torch.nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_token], dim=-1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = Model()\n",
    "optimizer = torch.optim.Adam(lm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 2.2891223430633545\n",
      "step 1000, loss 2.351369619369507\n",
      "step 2000, loss 2.670475482940674\n",
      "step 3000, loss 2.4176013469696045\n",
      "step 4000, loss 2.1189658641815186\n",
      "step 5000, loss 2.4586892127990723\n",
      "step 6000, loss 2.5530014038085938\n",
      "step 7000, loss 2.399681806564331\n",
      "step 8000, loss 2.3939037322998047\n",
      "step 9000, loss 2.6133534908294678\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = lm(x, y)\n",
    "    if step % 1000 == 0:\n",
    "        print(f'step {step}, loss {loss.item()}')\n",
    "    lm.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIIZESithahe benssiakee ptaby pof n:\\nThaighik, hyounod ie gune, ofamavins th?\\nFofe than?\\nWhen te he, is, s, aved iasenspiour yot flfinkewaviceour je ct th aig batiopr, s l o me m:\\nTalend d brades hetthencor, TExe han tu her bopr't, won on T:\\nH:\\nFin t h, fo caraisthathm\\nINCAl t ed; ton S:\\nME:\\n\\nHERI notan wo th ar KI savin olvely IOM s fusthe vintof d d hout:\\n\\nWhe g ERGof l, theO ounofor by ir t sus n, t y m eseathaigand ce ye mintharis t fedor\\nBers, lode gh?-\\n\\nCKE ancoulth ddellise-\\nFe,\\n\\nFrutheclotore:\\n\\nUnothamas t ns hieanh apusovome to e n, EENCHed;\\ns the wove.\\nBRTr meavelll. g; ro thes h thas, w n RYCENESwh amecorot ncal thate, wong an ay me anos:\\n\\nNToro beaithitath s, peanoit buthas mpe s thenty mowhan torou ofantos. f tthouou myo--m che, lawofa wout fuceaus,\\nsy.\\nPrailgaveexatherous ast h pl othee, fr. these nlisherecou\\n\\nGLI t fothan toun w Wicofsh sthonoloran, ngonanthinesinaithalor se?\\nTond imid\\nLEENUCHe thefond IULAno h I amy he y tour.\\nButhriowre gat apreadithantwrch tholounacre \""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype=torch.int64)\n",
    "y_ = lm.generate(x, 1000)\n",
    "decode(y_[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
