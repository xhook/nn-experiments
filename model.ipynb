{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "input_file_path = 'shakespeare.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(sorted(set(data)))}\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_encoded = np.array([stoi[ch] for ch in train_data], dtype=np.uint16)\n",
    "val_data_encoded = np.array([stoi[ch] for ch in val_data], dtype=np.uint16)\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "n_embd = 32\n",
    "batch_size = 640\n",
    "block_size = 8\n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data_encoded\n",
    "    elif split == 'val':\n",
    "        data = val_data_encoded\n",
    "    else:\n",
    "        raise ValueError('split must be either train or val')\n",
    "    start_idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in start_idx])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in start_idx])\n",
    "    return x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[46, 58,  1,  ..., 59,  1, 42],\n",
       "         [56, 57, 43,  ...,  0, 18, 30],\n",
       "         [50,  1, 54,  ...,  1, 63, 53],\n",
       "         ...,\n",
       "         [58, 53,  1,  ..., 49, 43,  1],\n",
       "         [53, 52, 39,  ..., 44, 47, 52],\n",
       "         [44, 39, 51,  ..., 57,  1, 47]], device='cuda:0'),\n",
       " tensor([[58,  1, 63,  ...,  1, 42, 47],\n",
       "         [57, 43, 10,  ..., 18, 30, 21],\n",
       "         [ 1, 54, 59,  ..., 63, 53, 59],\n",
       "         ...,\n",
       "         [53,  1, 51,  ..., 43,  1, 46],\n",
       "         [52, 39, 11,  ..., 47, 52, 42],\n",
       "         [39, 51, 53,  ...,  1, 47, 57]], device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embd, head_size)\n",
    "        self.query = torch.nn.Linear(n_embd, head_size)\n",
    "        self.value = torch.nn.Linear(n_embd, head_size)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(block_size, block_size)) == 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (head_size**-0.5)\n",
    "        wei = wei.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        return wei @ v        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = torch.nn.Linear(n_heads*head_size, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embd, 4 * n_embd),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, n_emnd, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = torch.nn.Sequential(*[Block(n_embd, 4) for _ in range(3)])\n",
    "        self.lm_head = torch.nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:, -block_size:])\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_token], dim=-1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = Model().to(device)\n",
    "optimizer = torch.optim.AdamW(lm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 4.578716278076172\n",
      "step 1000, loss 1.9970357418060303\n",
      "step 2000, loss 1.9055207967758179\n",
      "step 3000, loss 1.8674724102020264\n",
      "step 4000, loss 1.8498947620391846\n",
      "step 5000, loss 1.8113723993301392\n",
      "step 6000, loss 1.79720139503479\n",
      "step 7000, loss 1.757796287536621\n",
      "step 8000, loss 1.8209480047225952\n",
      "step 9000, loss 1.7407855987548828\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = lm(x, y)\n",
    "    if step % 1000 == 0:\n",
    "        print(f'step {step}, loss {loss.item()}')\n",
    "    lm.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HENRY VI\n",
      "How in her,\n",
      "It him ass\n",
      "I am you has, my four coswere of Dishort but can daughter as in and loal my both, harm?\n",
      "Our tendle thou, stay, and Senfock chused mine,\n",
      "Wasted sweet with with shaments but I dead: subquartten the ewo order neess;\n",
      "Not of direin court ABETH:\n",
      "Uthere-movine us have all vate ever, maid renefore fly stummanden the for'l quato thus will timold better, More him?\n",
      "\n",
      "Shecties.\n",
      "Trough and but ourseen outh,\n",
      "And riden us been.\n",
      "\n",
      "QUEER:\n",
      "Welcomas.\n",
      "\n",
      "YENIUS:\n",
      "Now,\n",
      "For that wear guife.\n",
      "\n",
      "BRUTABENVOLIXENES:\n",
      "'Gre do sleep alpatchman:\n",
      "Herern! Seventence ruestrain you; now unneds?\n",
      "\n",
      "GLOUCESTER:\n",
      "Thou with our have have poil of chout with the shake to his women France hod, Vole ordreator gries; thou? all can I your look's his king?\n",
      "\n",
      "SYONNRENCE:\n",
      "For VI:\n",
      "Before, patol; alloa part.\n",
      "\n",
      "KING HENRY BOLINGBROMEO:\n",
      "A pursme despire, when stay,\n",
      "And cut?\n",
      "'Tis bless, oness of coxtently lord; thousand too; and deck the part.\n",
      "\n",
      "LEONTES:\n",
      "That prosys, who childly been, a sold good, keep whrone.\n",
      "\n",
      "MARCIU\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype=torch.int64).to(device)\n",
    "y_ = lm.generate(x, 1000)\n",
    "print(decode(y_[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Maraant bre in by be bode,\n",
      "And \n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([encode('Hi M')]).to(device)\n",
    "y_ = lm.generate(x, 30)\n",
    "print(decode(y_[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
